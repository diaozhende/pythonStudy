## 机器学习笔记

### 1、机器学习开发流程

![](C:\Users\diaozhende\Pictures\Saved Pictures\pythonStudy\机器学习开发流程.png)

```
获取数据-->数据处理-->特征工程-->机器学习算法训练-->模型评估-->应用
```

### 2、特征工程

#### （1）sklearn数据集

##### *1、API

![](C:\Users\diaozhende\Pictures\Saved Pictures\pythonStudy\sklearn数据集.png)

##### *2、获取数据

```python
# 导入数据集
from sklearn.datasets import load_iris
# 获取数据
# 小规模数据集：load_，大规模数据集：fetch_
data = load_iris()
# data:特征值
# target：目标值
# DESCR：数据描述
# feature_names：特征名
# target_names：标签名
```

##### *3、划分数据集

###### API

![](C:\Users\diaozhende\Pictures\Saved Pictures\pythonStudy\划分数据集.png)

```python
# 数据集的特征值，数据集的目标值，测试集的大小(20%)，随机数种子
x_train,x_test,y_train,y_test = train_test_split(data.data,data.target,test_size=0.2,random_state=22)
# 训练集的特征值，测试集的特征值，训练集的目标值，测试集的目标值
```

#### （2）特征抽取

##### *1、API

![](C:\Users\diaozhende\Pictures\Saved Pictures\pythonStudy\字典特征提取.png)

##### *2、字典特征抽取

```python
from sklearn.feature_extraction import DictVectorizer
data = [{'city': '北京', 'temperature': 100}, {'city': '上海', 'temperature': 60}, {'city': '深圳', 'temperature': 30}]
# 如果sparse=True的话返回的值是one-hot编码，如果是False的话返回稀疏矩阵(返回值在one-hot编码中的位置)，默认值是False
trainfer = DictVectorizer(sparse=True)
dict_data = trainfer.fit_transform(data)
print(dict_data.toarray(),type(dict_data))
# 特征名称
print("特征名称:\n",trainfer.get_feature_names())
```



##### *2、文本特征抽取

###### API

![](C:\Users\diaozhende\Pictures\Saved Pictures\pythonStudy\文本特征抽取.png)

```python
data_text = ["life is short,i like like python", "life is too long,i dislike python"]
from sklearn.feature_extraction.text import CountVectorizer
# stoo_words:停用词
trainfer = CountVectorizer(stop_words=["is","too"])
data_new = trainfer.fit_transform(data_text)
print(data_new.toarray())
print("特征名称：",trainfer.get_feature_names())
```

##### *3、使用TF-IDF对文本进行抽取

###### 1）API

![](C:\Users\diaozhende\Pictures\Saved Pictures\pythonStudy\TFIDF特征抽取.png)

###### 2）结巴分词

```python
import jieba
jieba.cut(data)
```

###### 3）文本抽取

```python
data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。","我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。","如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
data_new = []
for item in data:
    data_new.append(jieba_cut(item))
trainfer = TfidfVectorizer()
result_data = trainfer.fit_transform(data_new)
print(result_data.toarray())
print("特征名称:\n",trainfer.get_feature_names())
```

#### （3）特征预处理

##### *1、无量纲化

###### 1）归一化

API

![](C:\Users\diaozhende\Pictures\Saved Pictures\pythonStudy\归一化API.png)

```python
# 1、读取数据
dat_data = pandas.read_csv("E:/python大数据资料/黑马-02-最简单快速入门python机器学习/资料/机器学习day1资料/02-代码/dating.txt")
# 2、实例一个转换器
from sklearn.preprocessing import MinMaxScaler
trainfrom = MinMaxScaler()
# 3、调用fit_trainfrom
new_data = trainfrom.fit_transform(dat_data)
```

###### 2）标准化

API：

![](C:\Users\diaozhende\Pictures\Saved Pictures\pythonStudy\标准化API.png)

```python
# 1、读取数据
import pandas
dat_data = pandas.read_csv("E:/python大数据资料/黑马-02-最简单快速入门python机器学习/资料/机器学习day1资料/02-代码/dating.txt")
# 2、实例一个转换器
from sklearn.preprocessing import StandardScaler
trainfer = StandardScaler()
# 3、调用fit_trainform
new_data = trainfer.fit_transform(dat_data)
```

#### （4）特征降维

##### *1、特征选择

###### API

![](C:\Users\diaozhende\Pictures\Saved Pictures\pythonStudy\特征选择降维API.png)

```python
# 1、读取数据
data = pandas.read_csv("E:/python大数据资料/黑马-02-最简单快速入门python机器学习/资料/机器学习day1资料/02-代码/factor_returns.csv")
# 去除第一列和倒数第二列，去除不符合数据类型的数据，数据类型必须是数字类型
data = data.iloc[:,1:-2]
print(data.shape)
# 2、实例一个转换器
from sklearn.feature_selection import VarianceThreshold
trainfrom = VarianceThreshold(threshold=10)
# 3、调用fit_trainfrom
new_data = trainfrom.fit_transform(data)
print(new_data)
print(new_data.shape)
```

##### *2、计算两个变量之间的相关系数

```python
# 计算某两个变量之间的相关系数
from scipy.stats import pearsonr
r1 = pearsonr(data["pe_ratio"], data["pb_ratio"])
print("相关系数：\n",r1)
r2 = pearsonr(data['revenue'], data['total_expense'])
print("revenue与total_expense之间的相关性：\n", r2)
```



##### *3、主成分分析(PCA降维)

###### API

![](C:\Users\diaozhende\Pictures\Saved Pictures\pythonStudy\PCA降维API.png)

```python
# 1、准备数据
data = [[2, 8, 4, 5], [6, 3, 0, 8], [5, 4, 9, 1]]
# 2、实例一个转换器
from sklearn.decomposition import PCA
# n_components:当数值为小数是表示保留对少数据，当数值为整数的时候表示降低到多少特征
pca = PCA(n_components=3)
# 3、调用fit_trainfrom
new_data = pca.fit_transform(data)
```

### 5、knn算法

#### （1）API

![](C:\Users\diaozhende\Pictures\Saved Pictures\pythonStudy\knn算法.png)

#### （2）使用knn算法对鸢尾花数据集进行分类

```python
# 1、读取数据
from sklearn.datasets import load_iris
iris_data = load_iris()
# 2、数据集的划分
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(iris_data.data,iris_data.target)
# 3、特征工程（标准化）
from sklearn.preprocessing import StandardScaler
transform = StandardScaler()
transform.fit_transform(x_train)
transform.transform(x_test)
# 4、knn算法估计器
from sklearn.neighbors import KNeighborsClassifier
# n_neighbors是knn的k值
knn = KNeighborsClassifier(n_neighbors=3)
# 训练数据
knn.fit(x_train,y_train)
# 5、模型评估
# 方法1：直接对比真实值和预估值
x_predict = knn.predict(x_test)
print("x_predict:\n",x_predict)
print("直接对比真实值和预估值:\n",y_test == x_predict)
# 方法2：计算准确率
score = knn.score(x_test,y_test)
print("准确率：",score)
```



###  6、模型选择和调优

#### （1）API

![](C:\Users\diaozhende\Pictures\Saved Pictures\pythonStudy\模型选择与调优.png)

#### （2）对鸢尾花数据集进行调优

```python
# 1、读取数据
from sklearn.datasets import load_iris
iris_data = load_iris()
# 2、数据集的划分
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(iris_data.data,iris_data.target)
# 3、特征工程（标准化）
from sklearn.preprocessing import StandardScaler
transform = StandardScaler()
transform.fit_transform(x_train)
transform.transform(x_test)
# 4、knn算法估计器
from sklearn.neighbors import KNeighborsClassifier
# n_neighbors是knn的k值
knn = KNeighborsClassifier(n_neighbors=3)

#添加网格搜索和交叉验证
from sklearn.model_selection import GridSearchCV
# knn中的k值，批量验证哪个k值最佳
param_dict = {"n_neighbors":[1, 3, 5, 7, 9, 11]}
# cv:进行几折交叉验证
knn = GridSearchCV(knn,param_grid=param_dict,cv=10)

# 训练数据
knn.fit(x_train,y_train)
# 5、模型评估
# 方法1：直接对比真实值和预估值
x_predict = knn.predict(x_test)
print("x_predict:\n",x_predict)
print("直接对比真实值和预估值:\n",y_test == x_predict)
# 方法2：计算准确率
score = knn.score(x_test,y_test)
print("准确率：",score)
# 最佳参数
print("最佳参数:\n",knn.best_params_)
# 最佳结果
print("最佳结果:\n",knn.best_score_)
# 最佳估计器
print("最佳估计器:\n",knn.best_estimator_)
# 交叉验证结果：cv_results_
print("交叉验证结果：\n",knn.cv_results_)
```

### 7、朴素贝叶斯算法

#### （1）API

![](C:\Users\diaozhende\Pictures\Saved Pictures\pythonStudy\朴素贝叶斯.png)

#### （2）用朴素贝叶斯算法对新闻数据进行分类

```python
# 1、读取新闻数据
from sklearn.datasets import fetch_20newsgroups
# data_home:指定数据的下载目录
# subset:数据类型，all(测试集和训练集)，train(训练集),test(测试集)
new_data = fetch_20newsgroups(data_home="D:/news/",subset="all")
# 2、划分数据集
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(new_data.data,new_data.target)
# 3、特征工程
# 用TfIdf进行文本特征抽取
from sklearn.feature_extraction.text import TfidfVectorizer
transform = TfidfVectorizer()
x_train = transform.fit_transform(x_train)
x_test = transform.transform(x_test)
# 4、贝叶斯算法预估器流程
from sklearn.naive_bayes import MultinomialNB
bayes = MultinomialNB(alpha=1.0)
bayes.fit(x_train,y_train)
score = bayes.score(x_test,y_test)
print("准确率：",score)
```

### 7、决策树

#### （1）API

![](C:\Users\diaozhende\Pictures\Saved Pictures\pythonStudy\决策树API.png)

#### （2）使用决策树对鸢尾花数据集进行分类

```python
# 1、读取数据
from sklearn.datasets import load_iris
iris_data = load_iris()
# 2、划分数据集
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(iris_data.data,iris_data.target)
# 3、特征工程（标准化）
from sklearn.preprocessing import StandardScaler
estimator = StandardScaler()
x_train = estimator.fit_transform(x_train)
x_test = estimator.transform(x_test)
# 4、决策树预估器
from sklearn.tree import DecisionTreeClassifier
estimator = DecisionTreeClassifier(criterion="entropy")
# 5、训练数据
estimator.fit(x_train,y_train)
# 6、计算准确率
score = estimator.score(x_test,y_test)
print("准确率为:\n",score)
```



### 8、随机森林

#### （1）API

![](C:\Users\diaozhende\Pictures\Saved Pictures\pythonStudy\随机森林API.png)

#### （2）使用随机森林对泰坦尼克号乘客生存进行预测

```python
# 1、读取数据
import pandas
titanic_data = pandas.read_csv("E:/python大数据资料/黑马-02-最简单快速入门python机器学习/资料/机器学习day2资料/02-代码/titanic.csv")
# 2、数据处理
# （1）筛选特征值和目标值
x = titanic_data[["pclass", "age", "sex"]]
y = titanic_data["survived"]
# （2）数据缺失值处理
pandas.notnull(x).all()
x["age"].fillna(x["age"].mean(),inplace=True)
# （3）将特征值换转成字典类型
x = x.to_dict(orient="records")
# 3、划分数据集
x_train,x_test,y_train,y_test = train_test_split(x,y)
# 4、特征工程(字典特征抽取)
from sklearn.feature_extraction import DictVectorizer
estimator = DictVectorizer()
x_train = estimator.fit_transform(x_train)
x_test = estimator.transform(x_test)
# 5、随机森林预估器流程
from sklearn.ensemble import RandomForestClassifier
estimator = RandomForestClassifier()
# 加入网格搜索与交叉验证
# 参数准备
from sklearn.model_selection import GridSearchCV
param_dict = {"n_estimators":[120,200,300,500,800,1200],"max_depth":[5,8,15,25,30]}
estimator = GridSearchCV(estimator,param_grid=param_dict,cv=3)
estimator.fit(x_train,y_train)
# 6、计算准确率
score = estimator.score(x_test,y_test)
print("准确率:\n",score)
```

### 9、线性回归

#### （1）API

![](C:\Users\diaozhende\Pictures\Saved Pictures\pythonStudy\线性回归.png)

#### （2）线性回归--正规方程

```python
# 1、读取数据
from sklearn.datasets import load_boston
boston_data = load_boston()
# 2、划分数据集
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(boston_data.data,boston_data.target)
# 3、特征工程(标准化)
from sklearn.preprocessing import StandardScaler
estimator = StandardScaler()
x_train = estimator.fit_transform(x_train)
x_test = estimator.transform(x_test)
# 4、线性回归预估器流程
from sklearn.linear_model import LinearRegression
estimator = LinearRegression()
# 5、训练数据
estimator.fit(x_train,y_train)
# 6、得出模型
print("正规方程-权重系数:\n",estimator.coef_)
print("正规方程-偏置:\n",estimator.intercept_)
# 7、模型评估
x_predict = estimator.predict(x_test)
print("预测房价:\n",x_predict)
# 正规方程的方差
from sklearn.metrics import mean_squared_error
error = mean_squared_error(y_test,x_predict)
print("正规方程的均方误差为:\n",error)
```



#### （3）线性回归--梯度下降

```python
# 1、读取数据
from sklearn.datasets import load_boston
boston_data = load_boston()
# 2、划分数据集
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(boston_data.data,boston_data.target)
# 3、特征工程(标准化)
from sklearn.preprocessing import StandardScaler
estimator = StandardScaler()
x_train = estimator.fit_transform(x_train)
x_test = estimator.transform(x_test)
# 4、梯度下降预估器流程
from sklearn.linear_model import SGDRegressor
estimator = SGDRegressor(learning_rate="constant",eta0=0.01,max_iter=10000,penalty="l1")
estimator.fit(x_train,y_train)
# 5、得出模型
print("梯度下降-权重系数:\n",estimator.coef_)
print("梯度下降-偏置:\n",estimator.intercept_)
# 6、模型评估
x_predict = estimator.predict(x_test)
print("预测房价:\n",x_predict)
from sklearn.metrics import mean_squared_error
error = mean_squared_error(y_test,x_predict)
print("均方误差:\n",error)
```

### 10、岭回归

#### （1）岭回归对博波士顿房价进行预测

```python
# 1、读取数据
from sklearn.datasets import load_boston
boston_data = load_boston()
# 2、划分数据集
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(boston_data.data,boston_data.target)
# 3、特征工程(标准化)
from sklearn.preprocessing import StandardScaler
estimator = StandardScaler()
x_train = estimator.fit_transform(x_train)
x_test = estimator.transform(x_test)
# 4、岭回归预估器
from sklearn.linear_model import Ridge
estimator = Ridge(alpha=0.5,max_iter=10000)
# 5、训练数据
estimator.fit(x_train,y_train)
# 6、得出模型
print("岭回归-权重系数为：\n", estimator.coef_)
print("岭回归-偏置为：\n", estimator.intercept_)

# 7、模型评估
y_predict = estimator.predict(x_test)
print("预测房价：\n", y_predict)
error = mean_squared_error(y_test, y_predict)
print("岭回归-均方误差为：\n", error)
```

### 11、逻辑回归

#### （1）API

![](C:\Users\diaozhende\Pictures\Saved Pictures\pythonStudy\逻辑回归.png)

#### （2）使用逻辑回归对癌症进行预测

```python
import pandas
import numpy
# 1、读取数据
path = "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"
column_name = ['Sample code number', 
               'Clump Thickness', 
               'Uniformity of Cell Size', 
               'Uniformity of Cell Shape',
               'Marginal Adhesion', 
               'Single Epithelial Cell Size',
               'Bare Nuclei',
               'Bland Chromatin',
               'Normal Nucleoli', 
               'Mitoses',
               'Class']

data = pandas.read_csv(path, names=column_name)
# 2、处理数据
# （1）缺失值处理,将？->numpy.nan
data = data.replace(to_replace="?",value=numpy.nan)
pandas.notnull(data).all()
# （2）删除缺失值样本
data.dropna(inplace=True)
# 3、划分数据集
# 筛选特征值和目标值
x = data.iloc[:, 1:-1]
y = data["Class"]
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y)
# 4、特征工程(标准化)
from sklearn.preprocessing import StandardScaler
estimator = StandardScaler()
# 5、训练数据
x_train = estimator.fit_transform(x_train)
x_test = estimator.transform(x_test)
# 6、逻辑回归预估器流程
from sklearn.linear_model import LogisticRegression
estimator = LogisticRegression()
estimator.fit(x_train,y_train)
# 7、计算准确率
score = estimator.score(x_test,y_test)
print("准确率：\n",score)
```

### 12、k-means分类算法

#### （1）API

![](C:\Users\diaozhende\Pictures\Saved Pictures\pythonStudy\K-mean.png)

